# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16f9kyY1KklwAbTGl7amMzl0vWOnqCZlI
"""



print("Setting up your Web Dev Environment...")
!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1
!pip install -qU langchain-ollama langchain-core

import subprocess
import threading
import time
import re
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


def run_ollama():
    subprocess.Popen(["ollama", "serve"])

threading.Thread(target=run_ollama).start()
time.sleep(5)
!ollama pull llama3.2:1b > /dev/null 2>&1


llm = ChatOllama(model="llama3.2:1b", temperature=0.7)


system_prompt = (
    "You are an expert web developer. Your task is to generate a complete, "
    "single-file website using HTML, CSS (internal), and JS (internal). "
    "Output ONLY the code inside a single markdown code block. "
    "Do not include any explanations before or after the code."
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "Create a modern website for: {user_topic} with html css and js all in a single file "),
])


chain = prompt | llm | StrOutputParser()


topic = input("Enter the website topic (e.g., 'A Pizza Shop' or 'A Space Portfolio'): ")
print(f"Generating website for '{topic}'...")

full_response = chain.invoke({"user_topic": topic})


code_match = re.search(r"```(?:html)?(.*?)```", full_response, re.DOTALL)
if code_match:
    clean_code = code_match.group(1).strip()
else:
    clean_code = full_response.strip()


with open("index.html", "w") as f:
    f.write(clean_code)

print("\n" + "="*30)
print("SUCCESS! File 'index.html' has been created.")
print("Check the folder icon on the left to download it.")
print("="*30)

from google.colab import drive
drive.mount('/content/drive')

# ==========================================
# STEP 1: INSTALL & SETUP OLLAMA
# ==========================================
print("Setting up environment (this takes ~1-2 mins)...")
!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1
!pip install -qU langchain-ollama langchain-community langchain pypdf faiss-cpu sentence-transformers

import subprocess
import threading
import time
import os
from google.colab import files
from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# Start Ollama server in background
def run_ollama():
    subprocess.Popen(["ollama", "serve"])

threading.Thread(target=run_ollama).start()
time.sleep(5)
!ollama pull llama3.2:1b > /dev/null 2>&1

# ==========================================
# STEP 2: UPLOAD & PROCESS PDF
# ==========================================
print("\n[Action Required] Please upload your PDF file:")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

print(f"Indexng {file_name}...")
loader = PyPDFLoader(file_name)
docs = loader.load()

# Split PDF into readable chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

# Create Vector Database (The "Brain")
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
retriever = vectorstore.as_retriever()

# ==========================================
# STEP 3: CONSTRUCT THE RAG CHAIN
# ==========================================
llm = ChatOllama(model="llama3.2:1b", temperature=0)

system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer the question. "
    "If you don't know the answer, just say that you don't know. "
    "\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}"),
])

# Create the chains (The modern way)
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

# ==========================================
# STEP 4: CHAT LOOP
# ==========================================
print("\n" + "="*40)
print(f"CHAT READY: Ask about '{file_name}'")
print("Type 'exit' to stop.")
print("="*40 + "\n")

while True:
    query = input("User: ")
    if query.lower() in ['exit', 'quit', 'stop']:
        print("Chat session ended.")
        break

    # Run the chain
    result = rag_chain.invoke({"input": query})
    print(f"\nAI: {result['answer']}\n")

!pip install langchain-classic

# ==========================================
# STEP 1: SETUP (Same as before)
# ==========================================
print("Setting up environment...")
!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1
!pip install -qU langchain-ollama langchain-community langchain-classic pypdf faiss-cpu sentence-transformers

import subprocess
import threading
import time
from google.colab import files
from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# NEW IMPORTS FOR v1.0+
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# Start Ollama
def run_ollama():
    subprocess.Popen(["ollama", "serve"])
threading.Thread(target=run_ollama).start()
time.sleep(5)
!ollama pull llama3.2:1b > /dev/null 2>&1

# ==========================================
# STEP 2: UPLOAD & PROCESS
# ==========================================
print("\n[Action] Upload your PDF:")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

loader = PyPDFLoader(file_name)
docs = loader.load()
splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)
vectorstore = FAISS.from_documents(documents=splits, embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"))

# ==========================================
# STEP 3: THE CHAIN (Updated Imports)
# ==========================================
llm = ChatOllama(model="llama3.2:1b", temperature=0)

prompt = ChatPromptTemplate.from_messages([
    ("system", "Use the context to answer: {context}"),
    ("human", "{input}"),
])

# These now pull from langchain_classic
combine_docs_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)

# ==========================================
# STEP 4: CHAT LOOP
# ==========================================
print("\n" + "="*40 + "\nCHAT READY! Type 'exit' to stop.\n" + "="*40)

while True:
    query = input("User: ")
    if query.lower() in ['exit', 'quit', 'stop']: break

    # Use 'input' key for create_retrieval_chain
    result = rag_chain.invoke({"input": query})
    print(f"\nAI: {result['answer']}\n")

!pip install -U langchain langchain-community

from langchain_community.llms import Ollama

llm = Ollama(
    model="llama3",
    temperature=0.7
)

response = llm.invoke("Explain gradient vs directional derivative.")
print(response)